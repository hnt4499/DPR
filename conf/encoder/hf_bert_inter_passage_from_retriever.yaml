# @package _group_

# model type. One of [hf_bert, pytext_bert, fairseq_roberta]
encoder_model_type: hf_bert_inter_passage_from_retriever

# HuggingFace's config name for model initialization
pretrained_model_cfg: bert-base-uncased

# Number of layers for each of the components; usually their sum is 12
num_layers: [12, 2]

# Some encoders need to be initialized from a file
pretrained_file:

# The main encoder might be initialized separately from a retriever model
pretrained_encoder_file: /data/tuyen/openqa/DPR/outputs_named/3_single_model/checkpoints/dpr_biencoder.31
encoder_initialize_from: question_encoder  # one of [question_encoder, ctx_encoder]
encoder_freeze: False  # whether to freeze the main encoder
encoder_learning_rate: 1e-6  # overwrite the default learning rate for the main encoder
allow_embedding_size_mismatch: True  # whether to truncate the pretrained embeddings to match the encoder embeddings

# Extra linear layer on top of standard bert/roberta encoder
projection_dim: 0

# Max length of the encoder input sequence
sequence_length: 256

dropout: 0.1

# whether to fix (don't update) context encoder during training or not
fix_ctx_encoder: False

# if False, the model won't load pre-trained BERT weights
pretrained: True