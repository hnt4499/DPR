
# configuration groups
defaults:
  - encoder: hf_bert
  - train: biencoder_default

# ============================ DATA =============================

# glob expression for train data files
train_files:
compress: False
shuffle_positives: True
train_iterator_class: ShardedDataIterator

# glob expression for dev data files
dev_files:
dev_iterator_class: ShardedDataIterator

# Entire tokenized Wikipedia passages (pkl format)
wiki_psgs_tokenized:

# ============================ OTHERS =============================

debugging: False

output_dir: checkpoints
checkpoint_file_name: dpr_biencoder
checkpoint_start_epoch: 30  # epoch to start checkpointing (i.e., saving model)

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

fix_ctx_encoder: False
val_av_rank_start_epoch: 30
seed: 12345

# A trained bi-encoder checkpoint file to initialize the model
model_file:
# These configs are only applicable when `model_file` is specified
resume: True  # whether to resume training
eval_first: True  # whether to evaluate the loaded model straightaway before any training
ignore_pretrained_model_type: False

# TODO: move to a conf group
# local_rank for distributed training on gpus
local_rank: -1
global_loss_buf_sz: 592000
device:
distributed_world_size:
distributed_port:
no_cuda: False
n_gpu:
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# Gradient checkpointing settings
gradient_checkpointing: False

# tokens which won't be slit by tokenizer
special_tokens:
