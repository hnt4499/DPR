# @package _group_

# General config
batch_size: 8
dev_batch_size: 32
# `batch_size` and `dev_batch_size` are the effective batch sizes, as well as the batch size for the biencoder model
# However, for the reader model, each batch of `batch_size` (or `dev_batch_size`) will be divided into `reader_num_sub_batches`
# sub-batches (for example, if `batch_size==8` and `reader_num_sub_batches==4` then `reader_batch_size==2`). This is required
# since batch size for reader model is usually smaller than batch size for biencoder model. Default value of 4 is good enough.
reader_num_sub_batches: 4

lamb: false
adam_eps: 1e-8
adam_betas: (0.9, 0.999)
max_grad_norm: 2.0
log_batch_step: 100
train_rolling_loss_step: 100
weight_decay: 0.0
num_train_epochs: 40
eval_per_epoch: 1

biencoder_learning_rate: 2e-5
reader_learning_rate: 1e-5

# Linear warmup over warmup_steps.
warmup_steps: 1237

# Number of updates steps to accumulate before performing a backward/update pass.
gradient_accumulation_steps: 1