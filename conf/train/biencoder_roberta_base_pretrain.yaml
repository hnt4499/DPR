# Hyperparameters as specified in the RoBERTa paper (Table 9 in the appendix)
# used for pretraining

batch_size:  # must be specified; should be close to 8k
dev_batch_size:  # must be specified
adam_eps: 1e-6
adam_betas: (0.9, 0.98)
max_grad_norm: null
log_batch_step: 100
train_rolling_loss_step: 100
weight_decay: 0.01
learning_rate: 0.0006

# Linear warmup over warmup_steps.
warmup_steps: 24000

# Number of updates steps to accumulate before performing a backward/update pass.
gradient_accumulation_steps: 1

# Total number of training epochs to perform.
num_train_epochs:  # must be specified; make sure `num_train_epochs` is appropriate so the total training steps is around 500k
eval_per_epoch: 1
hard_negatives: 1
other_negatives: 0
val_av_rank_hard_neg: 30
val_av_rank_other_neg: 30
val_av_rank_bsz: 128
val_av_rank_max_qs: 10000
